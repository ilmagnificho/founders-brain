---
video_id: BJjsfNO5JTo

How To Get The Most Out Of Vibe Coding  Startup School

AI can't yet one-shot an entire productâ€”but with the rise of vibe coding, it's getting close.  

YC's Tom Blomfield has spent the last month building side projects with tools like Claude Code, Windsurf, and Aqua, seeing just how far you can push modern LLMs. From writing full-stack apps to debugging with a single paste of an error message, AI is becoming a legit collaborator in the dev process. This video is a playbook for anyone who wants to get the most out of vibe coding and build faster.
---
Intro
0:01
[Music]
0:09
Hi, I'm Tom and I'm a partner here at
0:11
YC. For the last month, I've been
0:14
experimenting vibe coding, a couple of
0:16
side projects and I found not only is it
0:19
remarkably good, but it's also a
0:21
practice you can get measurably better
0:23
at if you're open to tinkering and
0:25
picking up best practices. In this
0:27
video, I want to share some ways you can
0:29
get great results when vibe coding. It's
0:31
kind of like uh prompt engineering from
0:34
a year or two ago. People were
0:35
discovering new stuff every week and
0:37
posting about it on social media. The
0:40
best techniques are the same techniques
0:42
that a professional software engineer
0:44
might use. And some people like, well,
0:46
that's not vibe coding, is it? You're
0:48
now just software engineering. I I I
0:50
kind of think that's beside the point.
0:51
We're trying to use these tools to get
0:53
the best results. And the YC Spring
Some vibe coding tips from YC X25 founders
0:55
Batch just kicked off a couple of weeks
0:57
ago. And before I give you my advice for
0:59
Vibe Coding, let's hear from the
1:01
founders on the tips they're using to
1:03
get the best out of the AI tools today.
1:05
If you get stuck in a place where the AI
1:06
ID can't implement or can't debug
1:08
something and it's just stuck in a loop,
1:10
sometimes going to the LLM's website,
1:13
like literally to the UI and just
1:14
pasting in your code and asking the same
1:16
question can get you a result that for
1:18
whatever reason the ID like couldn't get
1:19
to and you can solve your problem that
1:21
way. So I'd say like just load up both
1:23
cursor and windsurf on the same project.
1:25
Um cursor it's a bit faster so you can
1:28
do a lot of like the front end like a
1:30
little more like full stacky like link
1:31
the front end to the back end. Windsurf
1:33
thinks for a bit longer. I used to just
1:35
be like scrolling on my phone while I
1:36
type build this agent or like you know
1:38
like like modify this prompt and I'll
1:40
just like scroll fix scroll or like you
1:42
know paste an error. Now while I'm
1:44
waiting for uh winds surf to think I can
1:46
go on cursor and like you know start
1:48
updating the front end. Uh, sometimes
1:49
I'll load up both at the same time and
1:51
like have like the same context. Maybe
1:53
if I'm trying to update the front end,
1:54
I'll give it like style it in like the
1:56
style of that file and then I'll just
1:58
press enter for both and then they'll
2:00
both basically give me like slightly
2:02
different iterations of the same front
2:04
end and I'll just pick which one I like
2:05
better. My advice would be to uh think
2:07
of the AI as a different kind of
2:09
programming language and vibe coding as
2:11
being a different um a new type of
2:13
programming language. And so instead of
2:15
programming with code, you're
2:16
programming with language. And uh
2:19
because of that you kind of have to
2:21
provide a lot of the necessary context
2:23
and uh information in a very detailed
2:26
way if you want to get good results. I
2:28
usually start wipe coding in the reverse
2:30
direction that is first starting from
2:31
the test cases. I handcraft my test
2:34
cases. I don't use any LLMs to write my
2:36
test cases and once it is done I have
2:39
strong guard rules that my LLMs can uh
2:42
can follow for generating the code.
2:44
Right? and then they can freely generate
2:45
the code that they want to generate. And
2:47
once I see those green flags on my test
2:49
cases, the job is done. I don't need to
2:51
micromanage my code bases. I just I just
2:53
uh take overview about the modularity of
2:55
the code. Other than that, it's fine.
2:57
Yeah, I think it's very important to
2:59
first spend like unreasonable amount of
3:01
time uh in like a pure LLM to build out
3:04
like the scope and the actual
3:06
architecture of what you're trying to
3:08
build before offloading that to cursor
3:10
or any other kind of coding tool uh and
3:12
let it just like free run in the
3:14
codebase just random making up stuff
3:16
that doesn't really work. Um so make
3:18
sure you understand what the actual goal
3:19
of what you're building is. My advice
3:21
would be to really monitor whether the
3:24
LLM is falling into a rabbit hole when
3:26
it's answering your question. And if you
3:28
notice that it just keeps regenerating
3:30
code and it looks kind of funky. It's
3:33
not really able to figure it out. If
3:34
you're having to find yourself copy and
3:36
pasting error messages all the time, it
3:38
probably means something's gone uh ary
3:41
and you should take a step back, even
3:42
prompt the LLM and say, "Hey, you know,
3:45
let's take a step back and try to
3:46
examine basically why it's failing. Is
3:49
it a, you know, is it because you
3:51
haven't provided enough context for the
3:53
LM to be able to figure it out or have
3:55
you just gotten unlucky and it's unable
3:57
to do your request? The overarching
4:00
theme here is to make the LLM follow the
4:03
processes that a good professional
4:05
software developer would use. So, let's
4:07
dive in and explore some of the best
4:09
vibe coding advice I've seen. First,
First, pick your tools and make a plan
4:12
where to start. If you've never written
4:14
any code before, I would probably go for
4:17
a tool like Replet or Lovable. They give
4:19
you an easy to use visual interface and
4:22
it's a great way to try out new UIs
4:25
directly in code. Many product managers
4:28
and designers are actually going
4:29
straight to implementation of a new idea
4:32
in code rather than designing mock-ups
4:34
in something like Figma just because
4:35
it's so quick. But when I tried this, I
4:38
was impressed with the UIs. But tools
4:40
like Lovable started to struggle when I
4:43
wanted to more precisely modify backend
4:46
logic rather than just pure UI changes.
4:48
I'd change a button over here and the
4:50
backend logic would bizarrely change. So
4:53
if you've written code before, even if
4:55
you're a little bit rusty like me, you
4:57
can probably leap straight to tools like
4:59
Windsurf Cursor or Claude Code. Once
5:02
you've picked the tool you want to use,
5:04
the first step is not to dive in and
5:06
write code. Instead, I would work with
5:08
the LLM to write a comprehensive plan.
5:12
Put that in a markdown file inside your
5:14
project folder and keep referring back
5:16
to it. This is a plan that you develop
5:18
with the AI and you sort of step through
5:20
while you're implementing the project
5:22
rather than trying to oneshot the whole
5:23
thing. And so what I'd do after you've
5:26
created the first draft of this plan, go
5:28
through it, delete or remove things that
5:30
you don't like. You might mark certain
5:32
features explicitly as won't do too
5:34
complicated. And you might also like to
5:36
keep a section of ideas for later. You
5:39
know, to tell the LLM, look, I consider
5:41
this, but it's out of scope for now.
5:43
Once you've got that plan, work with the
5:45
LLM to implement it section by section
5:48
and explicitly say, let's just do
5:50
section two right now. Then you check
5:52
that it works. You run your tests and
5:54
you get commit. Then have the AI go back
5:56
to your plan and mark section two as
5:58
complete. I probably wouldn't expect the
6:00
models to oneshot entire products yet,
6:04
especially if they're complicated. I
6:06
prefer to do this piece by piece and
6:08
make sure I have a working
6:09
implementation of each step and
6:10
crucially commit it to Git so that you
6:12
can revert if if things go wrong on the
6:14
the next step. But honestly, this advice
6:17
might change in the next 2 or 3 months.
6:19
The models are getting better so quickly
6:22
that it's hard to say where we're going
6:23
to be in the near future. My next tip is
6:26
to use version control. Version control
Use version control
6:28
is your friend. Use Git religiously. I
6:32
know the tools have these kind of revert
6:34
sort of functionality. I don't trust
6:36
them yet. So, I always make sure I'm
6:38
starting with a kind of a clean git
6:39
slate before I start a new feature so
6:41
that I can revert to a known working
6:44
version if the AI goes off on a vision
6:46
quest. So, don't be afraid to get reset
6:49
head hard if it's not working and just
6:51
roll the dice again. I found I had bad
6:54
results if I'm like prompting the AI
6:57
multiple times to try to get something
6:58
working. It tends to accumulate layers
7:01
and layers and layers of bad code rather
7:04
than like really understanding the root
7:06
cause. You might go and and try four,
7:09
five, six different prompts and you
7:11
finally get the solution. I'd actually
7:12
just take that solution, get reset, and
7:15
then feed that solution into the AI on a
7:17
clean codebase so you can implement that
7:19
clean solution without layers and layers
7:21
of of craft. The next thing you should
Write tests
7:23
do is write tests or get your LLM to
7:26
write tests for you. They're pretty good
7:28
at this. Although they often default to
7:30
writing very low-level like unit tests,
7:33
I prefer to keep these tests super high
7:35
level. uh basically you want to simulate
7:38
someone clicking through the the site or
7:40
the app and ensure that the features are
7:43
working end to end rather than testing
7:45
uh functions on a kind of unit basis and
7:48
so make sure you write highlevel
7:50
integration tests before you move on to
7:52
the next feature. LLMs have a bad habit
7:55
of making unnecessary changes to
7:57
unrelated logic. So you tell it to fix
7:59
this thing over there and it just
8:01
changes some logic over here for really
8:02
no reason at all. And so having these
8:04
test suites in place catch these
8:06
regressions early will identify when the
8:09
LLM has has gone off and made
8:10
unnecessary changes so that you can get
8:13
reset and start again. Keep in mind LLMs
Remember, LLM's aren't just for coding
8:16
aren't just for coding. I use them for a
8:18
lot of non-coding work when I'm building
8:20
these kind of side projects. For
8:21
example, I had uh Claude Sonet 3.7
8:25
configure my DNS servers which is always
8:27
a task I hated and set up Heroku hosting
8:30
via a command line tool. It was a DevOps
8:32
engineer for me and accelerated my
8:34
progress like 10x. I also used chat GPT
8:37
to create a um an image for my site's
8:40
favicon, that little icon that appears
8:41
at the top of the the browser window.
8:44
And then Claude took that image and
8:46
wrote a quick throwaway script to resize
8:48
it into the six different sizes and
8:50
formats I needed for favicons across all
8:53
different platforms. So the AI is now my
8:55
designer as well. Okay, so now let's
Bug fixes
8:57
look at bug fixes. The first thing I do
9:00
when I encounter any bug is just copy
9:03
paste the error message straight back
9:05
into the LLM. It might be from your
9:07
server log files or the JavaScript
9:10
console in the browser. Often this error
9:13
message is enough for the AI to identify
9:15
and fix a problem. You don't even need
9:17
to explain what's going wrong or what
9:18
you think's going wrong. Simply the
9:20
error message is enough. It's so
9:22
powerful that pretty soon I actually
9:24
expect all the major coding tools to be
9:26
able to ingest these errors without
9:29
humans having to copy paste. If you
9:30
think about it, our value being the
9:32
copypaste machine is is uh kind of
9:35
weird, right? We're like we're we're
9:36
leaving the thinking to LLM. But I think
9:38
that copyping is going to go out the
9:39
window and these LLM tools are going to
9:41
be able to tail logs or you know um spin
9:44
up a headless browser and inspect the
9:46
the kind of JavaScript errors. With more
9:48
complex bugs, you can ask the LLM to
9:51
think through three or four possible
9:52
causes before writing any code. After
9:55
each failed attempt at fixing the bug, I
9:58
would get reset and start again again so
10:00
you're not accumulating layers and
10:01
layers of crust. Don't make multiple
10:03
attempts at bug fixes without resetting
10:06
because the LLM just adds more layers of
10:08
crap. Get reset, start again, and add
10:10
logging. Logging is your friend. If in
10:13
doubt, if it's not working, switch
10:15
models. Maybe it's Claude um Sonic 3.7,
10:18
maybe it's one of the OpenAI models,
10:20
maybe it's Gemini. I often find that um
10:23
different models succeed where the
10:25
others fail. And if you do eventually
10:27
find the source of a gnarly bug, I would
10:29
just reset all of the changes and then
10:32
um give the LLM very specific
10:34
instructions on how to fix that precise
10:36
bug on a clean code base to avoid this
10:39
like layers and layers of of junk code
10:41
accumulating. Next tip is to write
10:44
instructions for the LLM. Put these
10:46
instructions in whether it's cursor
10:48
rules, windsurf rules, claw markdown
10:51
file. Each tool has a slightly different
10:53
naming convention. But I know founders
10:55
who've written hundreds of lines of
10:57
instructions for uh their AI coding
11:00
agent and it makes them way way way more
11:02
effective. There's tons of advice online
11:04
about what to put in these instruction
11:06
files. I'll let you go and find that on
11:07
your own. Okay, let's talk about
11:09
documentation. I still find that
Documentation
11:12
pointing these agents at online web
11:14
documentation is a little bit patchy
11:17
still. I mean, some people are
11:18
suggesting uh using an MCP server to
11:21
access this
11:22
documentation, which works for some
11:24
people. Uh seems like overkill to me.
11:26
So, I'll often just download all of the
11:28
documentation for a given set of APIs
11:31
and put them in a subdirectory of my
11:33
working folder so the LLM can access
11:35
them locally. And then in my
11:37
instructions, I'll say go and read the
11:38
docs before you implement this thing.
11:40
And it's often much more accurate. A
11:42
side note to remember, you can use the
11:44
LLM as a teacher, especially for people
11:47
who are less familiar with the coding
11:48
language. You might implement something
11:50
and then get the AI to walk through that
11:52
implementation line by line and explain
11:54
it to you. It's a great way to learn new
11:57
technologies. It's much better than
11:59
scrolling Stack Overflow like uh we all
12:02
used to do. Now, let's look at more
Functionality
12:04
complex functionality. If you're working
12:06
on a a new piece of functionality, a new
12:08
feature that's more complex than you'd
12:10
normally trust the AI to implement, I
12:13
would do it as a standalone project in a
12:15
totally clean codebase. Get a small
12:19
reference implementation working without
12:21
the the complication of your existing
12:22
project or even download a reference
12:25
implementation if someone's written one
12:26
and posted on GitHub. Then you point to
12:29
your LLM at the implementation and tell
12:32
it to follow that while reimplementing
12:34
it inside your larger codebase. It
12:37
actually works surprisingly well.
12:39
Remember, small files and modularity are
12:42
your friend. This is true for human
12:44
coders as well. I think we might see a
12:46
shift towards more modular or
12:48
service-based architecture where the LLM
12:51
has clear API boundaries that it can
12:53
work within while maintaining a
12:55
consistent external interface rather
12:58
than these huge uh monor repos with
13:00
massive interdependencies. These are
13:02
hard for both humans and LLMs. It's just
13:05
not clear if a change in one place is
13:07
going to impact another part part of the
13:09
codebase. And so having this this modern
13:11
architecture with a consistent external
13:13
API means you can change the internals
13:16
as long as the external interface in the
13:18
test still pass you're probably good.
13:19
Now a note on choosing the right text
Choose the correct stack
13:21
stack. I chose to build my project
13:24
partially in Ruby on Rails mostly
13:26
because I was familiar with it from when
13:28
I used to um be a professional
13:30
developer. But I was blown away by the
13:32
AI's performance especially when it was
13:34
writing uh Rubon Rails code. And I think
13:37
this is because Rails is a 20-year-old
13:40
framework with a ton of wellestablished
13:42
conventions. A lot of Rails code bases
13:45
look very, very similar and it's obvious
13:47
to an experienced Ruby on Rails
13:49
developer where a specific piece of
13:50
functionality should live or the right
13:53
Rails way of achieving a certain
13:55
outcome. That means there's a ton of
13:57
pretty consistent highquality training
14:00
data for Rails code bases online. I've
14:03
had other friends have less success with
14:05
languages like Rust or Elixir where
14:07
there's just not as much training data
14:09
online, but who knows that might change
14:11
very soon. Okay, next bit of advice, use
14:15
screenshots. You can copy and paste
14:18
screenshots into most coding agents
14:19
these days and it's very useful either
14:22
to demonstrate a bug in the UI
14:24
implementation that you can see or to
14:26
pull in design uh inspiration from
14:30
another site that you might want to use
14:31
in your project. Voice is another really
14:35
cool way to interact with these tools. I
14:37
use Aqua, a YC company, and basically I
14:40
can just talk at my computer and Aqua
14:43
transcribes whatever I'm saying into the
14:46
tool I'm using. I'm switching a lot
14:48
between Windinsurf and Clawude Code at
14:49
the moment, but with Aqua, I can
14:52
effectively input instructions at 140
14:54
words per minute, which is about double
14:56
what I can type. And the AI is so
14:58
tolerant of minor grammar and
15:00
punctuation mistakes that it honestly
15:02
doesn't matter if the transcription's
15:04
not perfect. I actually wrote this
15:06
entire talk with Aqua. Next, make sure
Refactor frequently
15:09
to refactor frequently. When you've got
15:11
the code working and crucially the tests
15:13
implemented, you can refactor at will,
15:16
knowing that your tests are going to
15:17
catch any regressions. You can even ask
15:19
the LLM to identify parts of your
15:21
codebase that seem repetitive or might
15:23
be good candidates for refactoring. And
15:25
again, this is just a tip that any
15:27
professional software developer would
15:29
follow. You don't have um files that are
15:32
thousands of lines long. You keep them
15:34
small and modular. It makes it much
15:35
easier for both humans and LLMs to
15:37
understand what's going on. Finally,
15:39
keep experimenting. It seems like the
Keep experimenting!
15:42
state-of-the-art of this stuff changes
15:44
week by week. I try every new model
15:46
release to see which performs better in
15:48
each different scenario. Some are better
15:50
at debugging or long-term planning or
15:53
implementing features or refactoring.
15:54
For example, at the moment, Gemini seems
15:57
best for whole codebase indexing and
15:59
coming up with implementation plans,
16:01
while Sonet 3.7, to me at least, seems
16:03
like the leading contender to actually
16:05
implement the code changes. I tried GPT
16:08
4.1 just a couple of days ago, and
16:10
honestly, I I wasn't yet as impressed.
16:11
It just came back with me with too many
16:13
questions and actually got the
16:15
implementation wrong too many times. But
16:17
I'll try it again next week and I'm sure
16:19
things will have changed again. Thanks
Outro
16:21
for watching and I'd love it if you have
16:24
tips or tricks for getting the most out
16:26
of these models. Please share them in
16:27
the comments below.
16:31
[Music]